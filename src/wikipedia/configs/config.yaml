paths:
  dataset_json: data/wikipedia/train.json
  tripod_json: data/wikipedia/tripod.json
  output_dir: outputs_wikipedia
  wandb_project: wikipedia-modernbert

training:
  seed: 43
  batch_size: 16
  epochs: 5
  lr: 3e-5
  weight_decay: 0.01
  log_every: 50
  eval_every: 50
  hard_negatives: gpt # could be "gpt", "windows" or "no_windows"

model:
  base_model: jhu-clsp/ettin-encoder-1b
  temperature: 0.05
  pooling: mean

loss:
  type: nt # could be "simcse" or "mlm"